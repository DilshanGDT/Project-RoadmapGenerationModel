{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fd067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147282c",
   "metadata": {},
   "source": [
    "Graph Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f63182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(val):\n",
    "    try:\n",
    "        return float(val.replace('Â° N','').replace('Â° S','').replace('Â° E','').replace('Â° W','').strip())\n",
    "    except:\n",
    "        return 0.0  # or some default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18cacd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_json(json_path, distance_threshold_km=50):\n",
    "    data = json.load(open(json_path,'r',encoding='utf-8'))\n",
    "    # Build node lists\n",
    "    safari_nodes = []\n",
    "    location_nodes = {}  # name -> index\n",
    "    for s in data:\n",
    "        safari_nodes.append(s)\n",
    "        location_nodes.setdefault(s['extracted_features']['district'], None)\n",
    "\n",
    "    # map nodes to indices\n",
    "    idx = 0\n",
    "    node_index = {}  # key -> idx (for safari use title/id; for location use 'LOC:Name')\n",
    "    for s in safari_nodes:\n",
    "        key = f\"SAF:{s['id'] if 'id' in s else s['title']}\"\n",
    "        node_index[key] = idx; idx+=1\n",
    "    for loc in location_nodes.keys():\n",
    "        key = f\"LOC:{loc}\"\n",
    "        node_index[key] = idx; idx+=1\n",
    "\n",
    "    # features: numeric + tags multi-hot + TF-IDF of description (optional)\n",
    "    # numeric arrays\n",
    "    lat = []\n",
    "    lon = []\n",
    "    rating = []\n",
    "    review_count = []\n",
    "    descriptions = []\n",
    "    all_tags = []\n",
    "    safari_keys = []\n",
    "    for s in safari_nodes:\n",
    "        safari_keys.append(f\"SAF:{s.get('id', s['title'])}\")\n",
    "\n",
    "        lat.append(safe_float(s['extracted_features'].get('latitude', '0')))\n",
    "        lon.append(safe_float(s['extracted_features'].get('longitude', '0')))\n",
    "\n",
    "\n",
    "        rating.append(float(s.get('rating', 0.0)))\n",
    "        review_count.append(int(s.get('total_reviews', '0').replace(',', '')))\n",
    "\n",
    "        descriptions.append(s.get('description',''))\n",
    "        all_tags.append(s.get('tags', []))\n",
    "\n",
    "    # normalize numeric\n",
    "    num_feats = np.vstack([lat, lon, rating, np.log1p(review_count)]).T\n",
    "    num_feats = StandardScaler().fit_transform(num_feats)\n",
    "\n",
    "    # tags multi-hot\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    tag_feats = mlb.fit_transform(all_tags)\n",
    "\n",
    "    # description tfidf -> reduce to 50 dims\n",
    "    vect = TfidfVectorizer(max_features=300)\n",
    "    tfidf = vect.fit_transform(descriptions).toarray()\n",
    "    # maybe reduce dimensionality if needed\n",
    "    # combine features\n",
    "    safari_feat = np.hstack([num_feats, tag_feats, tfidf])\n",
    "\n",
    "    # build features for all nodes (safaris then locations)\n",
    "    N = idx\n",
    "    feat_dim = safari_feat.shape[1]\n",
    "    X = np.zeros((N, feat_dim), dtype=np.float32)\n",
    "    for i, key in enumerate(safari_keys):\n",
    "        X[node_index[key]] = safari_feat[i]\n",
    "\n",
    "    # location features: use mean of safaris in location or lat/lon\n",
    "    for loc in location_nodes.keys():\n",
    "        key = f\"LOC:{loc}\"\n",
    "        # compute mean over safaris at that location\n",
    "        saf_idxs = [node_index[f\"SAF:{s.get('id', s['title'])}\"] for s in safari_nodes if s['extracted_features']['district']\n",
    "==loc]\n",
    "        if saf_idxs:\n",
    "            X[node_index[key]] = safari_feat[[i for i,s in enumerate(safari_nodes) if s['extracted_features']['district']\n",
    "==loc]].mean(axis=0)\n",
    "        else:\n",
    "            X[node_index[key]] = np.zeros(feat_dim)\n",
    "\n",
    "    # build edges: safari->location and safari-safari similarity (tag overlap)\n",
    "    edges = []\n",
    "    for s in safari_nodes:\n",
    "        s_key = f\"SAF:{s.get('id', s['title'])}\"\n",
    "        loc_key = f\"LOC:{s['extracted_features']['district']}\"\n",
    "        edges.append((node_index[s_key], node_index[loc_key]))\n",
    "    # safari-safari edges by tag overlap\n",
    "    for i,s1 in enumerate(safari_nodes):\n",
    "        for j,s2 in enumerate(safari_nodes[i+1:], start=i+1):\n",
    "            if set(s1.get('tags',[])) & set(s2.get('tags',[])):\n",
    "                a = node_index[f\"SAF:{s1.get('id', s1['title'])}\"]\n",
    "                b = node_index[f\"SAF:{s2.get('id', s2['title'])}\"]\n",
    "                edges.append((a,b)); edges.append((b,a))\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    data = Data(x=X, edge_index=edge_index)\n",
    "    return data, node_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861675fa",
   "metadata": {},
   "source": [
    "GNN Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764feef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ac04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"safaris_30_items_enhanced.json\"\n",
    "data, node_index = build_graph_from_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98df7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3008f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Project-RoadmapGenerationModel\\venv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.3582, Val Acc: 1.0000\n",
      "Epoch 20, Loss: 0.3496, Val Acc: 1.0000\n",
      "Epoch 30, Loss: 0.4095, Val Acc: 1.0000\n",
      "Epoch 40, Loss: 0.3899, Val Acc: 1.0000\n",
      "Epoch 50, Loss: 0.3133, Val Acc: 1.0000\n",
      "Epoch 60, Loss: 0.4267, Val Acc: 1.0000\n",
      "Epoch 70, Loss: 0.3767, Val Acc: 1.0000\n",
      "Epoch 80, Loss: 0.3358, Val Acc: 1.0000\n",
      "Epoch 90, Loss: 0.3630, Val Acc: 1.0000\n",
      "Epoch 100, Loss: 0.4056, Val Acc: 1.0000\n",
      "âœ… Training finished!\n"
     ]
    }
   ],
   "source": [
    "# 1ï¸âƒ£ Split edges into train/val/test sets\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "# 2ï¸âƒ£ Define a simple GCN model for link prediction\n",
    "class GCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GCNLinkPredictor, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 3ï¸âƒ£ Decoder: dot product similarity\n",
    "def decode(z, edge_index):\n",
    "    z_src = z[edge_index[0]]\n",
    "    z_dst = z[edge_index[1]]\n",
    "    return (z_src * z_dst).sum(dim=1)  # dot product\n",
    "\n",
    "# 4ï¸âƒ£ Loss function\n",
    "def compute_loss(z, pos_edge_index, neg_edge_index):\n",
    "    pos_scores = decode(z, pos_edge_index)\n",
    "    neg_scores = decode(z, neg_edge_index)\n",
    "    labels = torch.cat([torch.ones(pos_scores.size(0)),\n",
    "                        torch.zeros(neg_scores.size(0))]).to(z.device)\n",
    "    scores = torch.cat([pos_scores, neg_scores])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "# 5ï¸âƒ£ Accuracy calculation\n",
    "def compute_accuracy(z, pos_edge_index, neg_edge_index):\n",
    "    pos_scores = torch.sigmoid(decode(z, pos_edge_index))\n",
    "    neg_scores = torch.sigmoid(decode(z, neg_edge_index))\n",
    "\n",
    "    preds = torch.cat([pos_scores, neg_scores])\n",
    "    labels = torch.cat([torch.ones(pos_scores.size(0)),\n",
    "                        torch.zeros(neg_scores.size(0))]).to(z.device)\n",
    "\n",
    "    pred_labels = (preds > 0.5).float()\n",
    "    correct = (pred_labels == labels).sum().item()\n",
    "    acc = correct / labels.size(0)\n",
    "    return acc\n",
    "\n",
    "# 6ï¸âƒ£ Model + optimizer\n",
    "model = GCNLinkPredictor(data.num_features, hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 7ï¸âƒ£ Training loop with validation accuracy\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model(data.x, data.train_pos_edge_index)\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    loss = compute_loss(z, data.train_pos_edge_index, neg_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ðŸ”¹ Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.train_pos_edge_index)  # embeddings\n",
    "        val_acc = compute_accuracy(z, data.val_pos_edge_index, data.val_neg_edge_index)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"âœ… Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5858faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recommend_safaris_by_district(query_district, model, data, node_index, node_features, safari_metadata, top_k=10):\n",
    "    \"\"\"\n",
    "    query_district: str - District name from user query (e.g., 'Ratnapura')\n",
    "    model: Trained GNN model\n",
    "    data: Graph data object\n",
    "    node_index: dict mapping node_id -> index in data.x\n",
    "    node_features: same as data.x before training\n",
    "    safari_metadata: dict mapping SAF:* -> safari JSON item\n",
    "    top_k: number of recommendations\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute embeddings for all nodes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.train_pos_edge_index)\n",
    "\n",
    "    # Step 2: Find safaris in the requested district\n",
    "    district_safaris = [\n",
    "        k for k, meta in safari_metadata.items()\n",
    "        if meta['extracted_features']['district'].lower() == query_district.lower()\n",
    "    ]\n",
    "    if not district_safaris:\n",
    "        raise ValueError(f\"No safaris found in district: {query_district}\")\n",
    "\n",
    "    # Step 3: Create pseudo-location vector by averaging district safari embeddings\n",
    "    district_indices = [node_index[k] for k in district_safaris]\n",
    "    loc_vec = z[district_indices].mean(dim=0, keepdim=True)  # shape: 1 x hidden\n",
    "\n",
    "    # Step 4: Score all safari nodes by dot product with location vector\n",
    "    safari_nodes = [k for k in node_index.keys() if k.startswith(\"SAF:\")]\n",
    "    safari_indices = [node_index[k] for k in safari_nodes]\n",
    "    scores = (z[safari_indices] * loc_vec).sum(dim=1)  # shape: num_safaris\n",
    "\n",
    "    # Step 5: Select top-k safaris\n",
    "    topk_scores, topk_idx = torch.topk(scores, k=min(top_k, len(scores)))\n",
    "    topk_safaris = [safari_nodes[i] for i in topk_idx]\n",
    "\n",
    "    # Step 6: Return results\n",
    "    results = [\n",
    "        {\n",
    "            \"title\": safari_metadata[saf_id]['title'],\n",
    "            \"district\": safari_metadata[saf_id]['extracted_features']['district'],\n",
    "            \"rating\": safari_metadata[saf_id]['rating'],\n",
    "            \"distance\": None,  # optional: you could compute distance using lat/lon\n",
    "            \"score\": float(topk_scores[j])\n",
    "        }\n",
    "        for j, saf_id in enumerate(topk_safaris)\n",
    "    ]\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
