{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84ca76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febefb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set up Base URL and Headers\n",
    "# Define a base URL without the offset parameter for pagination\n",
    "# The 'oaX' offset will be inserted dynamically\n",
    "base_url = 'https://www.tripadvisor.com/AttractionProductReview-g304141-d23824956-Private_Jeep_Safari_at_Minneriya_National_Park_to_Visit_Elephants-Sigiriya_Central.html'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Headers configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ffbb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Status Code: 200\n",
      "üì¶ Content Encoding: gzip\n",
      "üìÑ Content Type: text/html; charset=utf-8\n",
      "üìè Content Length: 1630445 bytes\n",
      "üî§ Detected encoding: utf-8\n",
      "üìù Response text length: 1630345 characters\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Session and Make Request\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "try:\n",
    "    response = session.get(base_url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    print(f\"‚úÖ Status Code: {response.status_code}\")\n",
    "    print(f\"üì¶ Content Encoding: {response.headers.get('Content-Encoding', 'None')}\")\n",
    "    print(f\"üìÑ Content Type: {response.headers.get('Content-Type', 'None')}\")\n",
    "    print(f\"üìè Content Length: {len(response.content)} bytes\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Request failed: {e}\")\n",
    "\n",
    "# Cell 4: Handle Text Encoding\n",
    "# Ensure proper encoding to avoid garbled text\n",
    "response.encoding = response.apparent_encoding or 'utf-8'\n",
    "\n",
    "print(f\"üî§ Detected encoding: {response.encoding}\")\n",
    "print(f\"üìù Response text length: {len(response.text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87075410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç≤ BeautifulSoup parsing completed\n",
      "üìä Found 5858 HTML elements\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "print(\"üç≤ BeautifulSoup parsing completed\")\n",
    "print(f\"üìä Found {len(soup.find_all())} HTML elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac02e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç First 500 characters of parsed content:\n",
      "==================================================\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\">\n",
      " <head>\n",
      "  <link href=\"https://static.tacdn.com/img2/brand_refresh_2025/application_icons/favicon_2025.ico\" id=\"favicon\" rel=\"icon\" type=\"image/x-icon\"/>\n",
      "  <link href=\"https://static.tacdn.com/img2/brand_refresh_2025/application_icons/icon.svg\" rel=\"icon\" type=\"image/svg+xml\"/>\n",
      "  <link href=\"https://static.tacdn.com/img2/brand_refresh_2025/application_icons/apple_touch_icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n",
      "  <link color=\"#00210c\" href=\"https://static.tacdn.com/img2/brand_refresh_2025/application_icons/mask_icon.svg\" rel=\"mask-icon\" sizes=\"any\"/>\n",
      "  <meta content=\"#00eb5b\" name=\"theme-color\"/>\n",
      "  <meta content=\"telephone=no\" name=\"format-detection\"/>\n",
      "  <meta content=\"app-id=-1\" name=\"apple-itunes-app\"/>\n",
      "  <script type=\"application/ld+json\">\n",
      "   [{\"@context\":\"https:\\u002F\\u002Fschema.org\",\"@type\":\"Organization\",\"name\":\"Tripadvisor\",\"url\":\"https:\\u002F\\u002Fwww.tripadvisor.com\\u002F\",\"logo\":\"https:\\u002F\\u002Fstatic.tacdn.com\\u002Fimg2\\u002Fbra\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test the Parsed Content\n",
    "# Display first 500 characters to verify it's readable\n",
    "print(\"üîç First 500 characters of parsed content:\")\n",
    "print(\"=\" * 50)\n",
    "print(soup.prettify()[:1000])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8600b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store all scraped places from all pages\n",
    "all_places_data = []\n",
    "\n",
    "# --- Pagination Control Variables ---\n",
    "offset = 0 # Starting offset for the first page\n",
    "total_items = float('inf') # Will be updated after scraping the first page\n",
    "items_per_page = 30 # Number of items displayed per page on Tripadvisor\n",
    "\n",
    "# Cell 3 & 4: Create Session, Make Request, Parse, and Extract (now within a loop for pagination)\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de77949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Current Loop Iteration ---\n",
      "Current offset before URL construction: 0\n",
      "https://www.tripadvisor.com/AttractionProductReview-g304141-d23824956-Private_Jeep_Safari_at_Minneriya_National_Park_to_Visit_Elephants-Sigiriya_Central.html\n",
      "Constructed URL for this page: https://www.tripadvisor.com/AttractionProductReview-g304141-d23824956-Private_Jeep_Safari_at_Minneriya_National_Park_to_Visit_Elephants-Sigiriya_Central.html\n",
      "Attempt 1 for URL: https://www.tripadvisor.com/AttractionProductReview-g304141-d23824956-Private_Jeep_Safari_at_Minneriya_National_Park_to_Visit_Elephants-Sigiriya_Central.html\n",
      "‚úÖ Status Code for https://www.tripadvisor.com/AttractionProductReview-g304141-d23824956-Private_Jeep_Safari_at_Minneriya_National_Park_to_Visit_Elephants-Sigiriya_Central.html: 200\n",
      "üç≤ BeautifulSoup parsing completed for current page\n",
      "‚ö†Ô∏è Could not find total number of items from the first page. Assuming only one page.\n",
      "Found 0 listings on this page.\n",
      "üö´ No new listings found on https://www.tripadvisor.com/AttractionProductReview-g304141-d23824956-Private_Jeep_Safari_at_Minneriya_National_Park_to_Visit_Elephants-Sigiriya_Central.html. This might be the last page or an empty page. Ending scrape.\n",
      "\n",
      "‚úÖ Scraped 0 items across all pages.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Loop through pages until all items are collected\n",
    "while offset < total_items:\n",
    "    \n",
    "    print(f\"\\n--- Current Loop Iteration ---\")\n",
    "    print(f\"Current offset before URL construction: {offset}\")\n",
    "\n",
    "    if offset == 0:\n",
    "        current_page_url = base_url\n",
    "        print(current_page_url)\n",
    "    else:\n",
    "        # Example: 'Attractions-g293961-Activities-c61-t243-oa30-Sri_Lanka.html'\n",
    "        # We split the base_url to insert the offset parameter correctly\n",
    "        parts = base_url.rsplit('-Sri_Lanka.html', 1)\n",
    "        current_page_url = f\"{parts[0]}-oa{offset}-Sri_Lanka.html\"\n",
    "        print(current_page_url)\n",
    "\n",
    "    print(f\"Constructed URL for this page: {current_page_url}\")\n",
    "\n",
    "    # --- MODIFIED: Add retry logic for requests ---\n",
    "    max_retries = 3 # Try up to 3 times for a failed request\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "\n",
    "        try:\n",
    "            print(f\"Attempt {attempt} for URL: {current_page_url}\") # New print for retry info\n",
    "            response = session.get(current_page_url, timeout=20) # <--- CHANGED: Increased timeout to 20 seconds\n",
    "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            print(f\"‚úÖ Status Code for {current_page_url}: {response.status_code}\")\n",
    "            break # Break out of the retry loop if request is successful\n",
    "\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            print(f\"‚ùå Timeout error on attempt {attempt} for {current_page_url}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = 5 * attempt # Exponential backoff: 5s, 10s, 15s delay\n",
    "                print(f\"Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"Max retries reached for {current_page_url}. Skipping this page.\")\n",
    "                response = None # Set response to None to indicate failure\n",
    "                break # Break out of retry loop if max attempts reached\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Request failed on attempt {attempt} for {current_page_url}: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = 5 * attempt # Exponential backoff\n",
    "                print(f\"Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"Max retries reached for {current_page_url}. Skipping this page.\")\n",
    "                response = None # Set response to None to indicate failure\n",
    "                \n",
    "    # If response is None after retries, skip processing this page and move to next offset\n",
    "    if response is None:\n",
    "        offset += items_per_page\n",
    "        time.sleep(2) # Still add a small delay before attempting next page\n",
    "        continue # Skip to the next iteration of the while loop\n",
    "\n",
    "    # Parse the HTML content for the current page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(\"üç≤ BeautifulSoup parsing completed for current page\")\n",
    "\n",
    "    # --- Extract Total Items from the first page (only once) ---\n",
    "    if offset == 0:\n",
    "        total_items_text_tag = soup.find('div', class_='Ci')\n",
    "        if total_items_text_tag:\n",
    "            full_text = total_items_text_tag.get_text(strip=True)\n",
    "            match = re.search(r'of\\s*([\\d,]+)', full_text)\n",
    "            if match:\n",
    "                total_items = int(match.group(1).replace(',', ''))\n",
    "                print(f\"üî¢ Total items identified for scraping: {total_items}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Could not find total number of items from the first page. Assuming only one page.\")\n",
    "                total_items = items_per_page # Fallback: if total items not found, only process the first page\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Total items text element not found. Assuming only one page.\")\n",
    "            total_items = items_per_page # Fallback: if element not found, only process the first page\n",
    "\n",
    "    # --- Data Extraction Logic (your existing robust extraction) ---\n",
    "    listings = soup.find_all('article', class_='GTuVU XJlaI rHoxO')\n",
    "    print(f\"Found {len(listings)} listings on this page.\")\n",
    "\n",
    "    if not listings:\n",
    "        print(f\"üö´ No new listings found on {current_page_url}. This might be the last page or an empty page. Ending scrape.\")\n",
    "        break # Exit loop if no listings are found on the current page\n",
    "\n",
    "    for listing in listings:\n",
    "        place_details = {}\n",
    "\n",
    "        # Extract Title and Detail Page URL (MODIFIED)\n",
    "        title_link_tag = listing.find('a') # Find the first <a> tag within the listing (expected to be the title link)\n",
    "        if title_link_tag:\n",
    "            # Ensure the h3 tag exists for title, as it's the primary text component\n",
    "            title_h3_tag = title_link_tag.find('h3', class_='biGQs _P fiohW OgHoE')\n",
    "            if title_h3_tag:\n",
    "                title_text = title_h3_tag.get_text(strip=True)\n",
    "                place_details['title'] = re.sub(r'^\\d+\\.\\s*', '', title_text).strip()\n",
    "            else:\n",
    "                place_details['title'] = 'N/A'\n",
    "\n",
    "            detail_page_relative_url = title_link_tag.get('href')\n",
    "            if detail_page_relative_url:\n",
    "                # Construct the full detail page URL\n",
    "                # Assuming base URL is consistent, might need more robust URL join\n",
    "                detail_page_url = f\"https://www.tripadvisor.com{detail_page_relative_url}\"\n",
    "                place_details['detail_page_url'] = detail_page_url # Optional: save the detail page URL too\n",
    "            else:\n",
    "                detail_page_url = None\n",
    "                place_details['detail_page_url'] = 'N/A' # Optional\n",
    "        else:\n",
    "            place_details['title'] = 'N/A'\n",
    "            detail_page_url = None\n",
    "            place_details['detail_page_url'] = 'N/A'\n",
    "\n",
    "        # Extract Image links\n",
    "        image_links = []\n",
    "        image_container = listing.find('div', class_='IdURT w carousel UznXc wSSLS')\n",
    "        if image_container:\n",
    "            img_tags = image_container.find_all('img', src=True)\n",
    "            for img_tag in img_tags:\n",
    "                src = img_tag['src']\n",
    "                if not src.startswith('data:image/svg+xml'):\n",
    "                    if src.startswith('//'):\n",
    "                        src = 'https:' + src\n",
    "                    image_links.append(src)\n",
    "        place_details['image_links'] = image_links if image_links else 'N/A'\n",
    "\n",
    "        # Extract Rating\n",
    "        rating_tag = listing.find('div', {'data-automation': 'bubbleRatingValue'})\n",
    "        if rating_tag:\n",
    "            place_details['rating'] = rating_tag.find('span').get_text(strip=True)\n",
    "        else:\n",
    "            place_details['rating'] = 'N/A'\n",
    "\n",
    "        # Extract Total Reviews\n",
    "        reviews_tag = listing.find('div', {'data-automation': 'bubbleLabel'})\n",
    "        if reviews_tag:\n",
    "            place_details['total_reviews'] = reviews_tag.get_text(strip=True)\n",
    "        else:\n",
    "            place_details['total_reviews'] = 'N/A'\n",
    "\n",
    "        # Extract Recommendation\n",
    "        recommendation_tag = listing.find('span', class_='biGQs _P pZUbB egaXP ZNjnF', string=lambda text: text and 'Recommended by' in text)\n",
    "        if recommendation_tag:\n",
    "            place_details['recommendation'] = recommendation_tag.get_text(strip=True)\n",
    "        else:\n",
    "            place_details['recommendation'] = 'N/A'\n",
    "\n",
    "        # Extract Type\n",
    "        type_div = listing.find('div', class_='alPVI eNNhq PgLKC tnGGX yzLvM')\n",
    "        if type_div:\n",
    "            type_element = None\n",
    "            all_text_divs = type_div.find_all('div', class_='biGQs _P pZUbB ZNjnF')\n",
    "            for div in all_text_divs:\n",
    "                if not div.find_parent('div', class_='bRMrl _Y K fOSqw') and \\\n",
    "                   not div.find_parent('div', {'data-automation': 'listCardDescription'}) and \\\n",
    "                   not div.find_parent('div', {'data-automation': 'cardPrice'}) and \\\n",
    "                   not \"cancellation\" in div.get_text(strip=True).lower() and \\\n",
    "                   not \"recommended by\" in div.get_text(strip=True).lower():\n",
    "                    type_element = div\n",
    "                    break\n",
    "            if type_element:\n",
    "                place_details['type'] = type_element.get_text(strip=True)\n",
    "            else:\n",
    "                place_details['type'] = 'N/A'\n",
    "        else:\n",
    "            place_details['type'] = 'N/A'\n",
    "\n",
    "        # Extract Travel Duration\n",
    "        duration_tag = listing.find('div', class_='bRMrl _Y K fOSqw')\n",
    "        if duration_tag:\n",
    "            duration_element = duration_tag.find('div', class_='biGQs _P pZUbB ZNjnF')\n",
    "            if duration_element:\n",
    "                place_details['travel_duration'] = duration_element.get_text(strip=True)\n",
    "            else:\n",
    "                place_details['travel_duration'] = 'N/A'\n",
    "        else:\n",
    "            place_details['travel_duration'] = 'N/A'\n",
    "\n",
    "        # Extract Description\n",
    "        # description_tag = listing.find('div', {'data-automation': 'listCardDescription'})\n",
    "        # if description_tag:\n",
    "        #     span_tag = description_tag.find('span', class_='SwTtt')\n",
    "        #     if span_tag:\n",
    "        #         description_text = span_tag.get_text(strip=True).replace('\\n', ' ').strip()\n",
    "        #         if not description_text.endswith('‚Ä¶') and '‚Ä¶' in span_tag.text:\n",
    "        #             description_text += '‚Ä¶'\n",
    "        #         place_details['description'] = description_text\n",
    "        #     else:\n",
    "        #         place_details['description'] = 'N/A'\n",
    "        # else:\n",
    "        #     place_details['description'] = 'N/A'\n",
    "\n",
    "        # Extract Starting Price\n",
    "        price_tag = listing.find('div', {'data-automation': 'cardPrice'})\n",
    "        if price_tag:\n",
    "            place_details['starting_price'] = price_tag.get_text(strip=True)\n",
    "        else:\n",
    "            place_details['starting_price'] = 'N/A'\n",
    "        pass\n",
    "        all_places_data.append(place_details)\n",
    "\n",
    "        # --- NEW LOGIC: Fetch and extract full description from detail page ---\n",
    "        if detail_page_url and detail_page_url != 'N/A':\n",
    "            print(f\"  ‚û°Ô∏è Fetching full description for: {place_details['title']}\")\n",
    "            time.sleep(2) # <--- IMPORTANT: Delay before fetching detail page\n",
    "            try:\n",
    "                detail_response = session.get(detail_page_url, timeout=15) # Shorter timeout for individual detail page\n",
    "                detail_response.raise_for_status()\n",
    "                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
    "\n",
    "                # Find the div with class '_d' for full description\n",
    "                full_description_tag = detail_soup.find('div', class_='_d')\n",
    "                if full_description_tag:\n",
    "                    place_details['description'] = full_description_tag.get_text(strip=True).replace('\\n', ' ')\n",
    "                else:\n",
    "                    place_details['description'] = 'Full Description N/A (selector not found)'\n",
    "                    print(f\"    ‚ùå Full description div ('_d') not found for {place_details['title']}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                place_details['description'] = f\"Full Description N/A (request failed: {e})\"\n",
    "                print(f\"    ‚ùå Failed to fetch detail page for {place_details['title']}: {e}\")\n",
    "        else:\n",
    "            place_details['description'] = 'Full Description N/A (detail URL missing)'\n",
    "\n",
    "        # Append the collected details for this place\n",
    "        all_places_data.append(place_details)\n",
    "\n",
    "    # Increment the offset for the next page\n",
    "    offset += items_per_page\n",
    "    print(f\"Offset for next page: {offset}\")\n",
    "\n",
    "    # Add a delay between requests to be polite and avoid being blocked\n",
    "    time.sleep(3) # IMPORTANT: Adjust this value as needed, typically 1-5 seconds\n",
    "\n",
    "# Final output: print all collected data after the loop finishes\n",
    "print(f\"\\n‚úÖ Scraped {len(all_places_data)} items across all pages.\")\n",
    "print(json.dumps(all_places_data, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
